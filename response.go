package openllm

import "time"

// Response wraps the final assistant message and any tool calls produced by the model.
// Both blocking and streaming APIs return a Response upon completion.
type Response interface {
	// Answer returns the final assistant message after generation finishes.
	Answer() Message
	// ToolCalls returns tool invocation records in the order they were produced.
	ToolCalls() []ToolCall
	// Usage returns the token usage statistics.
	// Notes:
	// - Blocking requests usually provide complete Usage (input/output tokens and cache-related metrics).
	// - Streaming requests in current SDKs typically do not expose Usage; usually empty or partial.
	Usage() Usage
	// Meta returns the request metadata (provider, model, request ID, etc.).
	Meta() Meta
	// Duration returns the total elapsed time of the request.
	Duration() time.Duration
}

// response is the concrete implementation of Response.
type response struct {
	// answer is the final assistant message constructed from the model output.
	answer Message
	// tcalls holds all function tool calls captured during generation.
	tcalls []ToolCall
	// usage captures token and cache-related consumption metrics.
	usage Usage
	// meta contains request metadata.
	meta Meta
	// duration captures the elapsed time from request start to completion.
	duration time.Duration
}

// Answer implements Response by returning the final assistant message.
func (resp *response) Answer() Message {
	return resp.answer
}

// ToolCalls implements Response by returning the collected tool calls.
func (resp *response) ToolCalls() []ToolCall {
	return resp.tcalls
}

// Usage implements Response.
func (resp *response) Usage() Usage {
	return resp.usage
}

// Meta implements Response.
func (resp *response) Meta() Meta {
	return resp.meta
}

// Duration implements Response.
func (resp *response) Duration() time.Duration {
	return resp.duration
}

// Usage captures token and cache-related consumption metrics.
type Usage struct {
	// number of input tokens (system, history, and user messages).
	InputTokens int
	// number of output tokens generated by the model.
	OutputTokens int
	// sum of input and output tokens.
	TotalTokens int
	// (OpenAI) tokens used for internal chain-of-thought processing before final answer.
	ReasoningTokens int
	// (OpenAI) total input tokens that were retrieved from the server-side cache.
	CachedTokens int
	// (Claude) input tokens charged for prompt cache creation (higher price).
	CacheCreationInputTokens int
	// (Claude) input tokens charged when reading from prompt cache (discounted).
	CacheReadInputTokens int
}

// Meta contains request metadata:
type Meta struct {
	// backend provider (e.g., openai, anthropic).
	Provider string
	// model name.
	Model string
	// request ID (useful for troubleshooting/auditing).
	RequestID string
	// (OpenAI) server fingerprint to distinguish backend versions.
	SystemFingerprint string
	// reason the generation stopped (e.g., stop_sequence, max_tokens, tool_use).
	StopReason string
}
