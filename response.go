package openllm

import "time"

// Response wraps the final assistant message and any tool calls produced by the model.
// Both blocking and streaming APIs return a Response upon completion.
type Response interface {
	// Answer returns the final assistant message after generation finishes.
	Answer() Message
	// ToolCalls returns tool invocation records in the order they were produced.
	ToolCalls() []ToolCall
	// Stats returns request-level statistics and metadata.
	// Notes:
	// - Blocking requests usually provide complete Usage (input/output tokens and cache-related metrics).
	// - Streaming requests in current SDKs typically do not expose Usage; only duration and basic metadata are available.
	//   If the underlying SDK supports streaming usage events (e.g., include_usage), we can aggregate Usage at the end.
	Stats() Stats
}

// response is the concrete implementation of Response.
type response struct {
	// answer is the final assistant message constructed from the model output.
	answer Message
	// tcalls holds all function tool calls captured during generation.
	tcalls []ToolCall
	// stats captures usage counters, duration, and provider/model metadata.
	stats Stats
}

// Answer implements Response by returning the final assistant message.
func (resp *response) Answer() Message {
	return resp.answer
}

// ToolCalls implements Response by returning the collected tool calls.
func (resp *response) ToolCalls() []ToolCall {
	return resp.tcalls
}

func (resp *response) Stats() Stats {
	return resp.stats
}

// Usage captures token and cache-related consumption metrics.
type Usage struct {
	// number of input tokens (system, history, and user messages).
	InputTokens int
	// number of output tokens generated by the model.
	OutputTokens int
	// sum of input and output tokens.
	TotalTokens int
	// (OpenAI) tokens used for internal chain-of-thought processing before final answer.
	ReasoningTokens int
	// (OpenAI) total input tokens that were retrieved from the server-side cache.
	CachedTokens int
	// (Claude) input tokens charged for prompt cache creation (higher price).
	CacheCreationInputTokens int
	// (Claude) input tokens charged when reading from prompt cache (discounted).
	CacheReadInputTokens int
}

// Meta contains request metadata:
type Meta struct {
	// backend provider (e.g., openai, anthropic).
	Provider string
	// model name.
	Model string
	// request ID (useful for troubleshooting/auditing).
	RequestID string
	// (OpenAI) server fingerprint to distinguish backend versions.
	SystemFingerprint string
	// reason the generation stopped (e.g., stop_sequence, max_tokens, tool_use).
	StopReason string
}

// Stats aggregates per-request metrics and metadata:
type Stats struct {
	// token/cache metrics (complete on blocking paths; may be empty for streaming depending on SDK support).
	Usage Usage
	// elapsed time from request start to completion.
	Duration time.Duration
	// provider, model, request identifiers, and stop reason.
	Meta Meta
}
