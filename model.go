package openllm

import (
	"context"
)

// StreamWatcher handles events emitted during LLM generation.
type StreamWatcher interface {
	OnRefusal(delta string) error

	OnReasoning(delta string) error

	// OnContent is invoked whenever the model emits a piece of streamed output.
	OnContent(delta string) error

	// OnToolCall is invoked when a tool call is detected in the stream.
	// It returns a function to write streamed arguments.
	OnToolCall(ctx context.Context, tcall ToolCall, args string) (err error)

	// OnStop is invoked after the model has finished producing all output.
	OnStop() error
}

// Model defines the abstract interface of an LLM runtime or engine.
// It encapsulates model identity, capability description, and
// chat-completion execution (both blocking and streaming).
type Model interface {
	// Name returns the unique, human-readable name of the LLM core.
	Name() string

	// Description returns a brief description of the LLM core.
	Description() string

	// ChatCompletion performs a blocking chat completion request.
	//
	// Parameters:
	//   - ctx:      Context for cancellation and timeout control
	//   - messages: Conversation history used as input
	//   - opts:     Optional configuration (setting, tools, event handler)
	//
	// Returns:
	//   - answer:    The final assistant message generated by the LLM
	//   - toolCalls: Tool calls requested by the LLM during generation
	//   - err:       Execution or communication error, if any
	ChatCompletion(ctx context.Context, messages []Message, opts ...ChatOption) (resp Response, err error)

	// ChatCompletionStream performs a streaming chat completion request.
	//
	// It behaves similarly to Chat but emits partial outputs via the StreamEventHandler
	// provided in options.
	ChatCompletionStream(ctx context.Context, messages []Message, opts ...ChatOption) (resp Response, err error)
}
